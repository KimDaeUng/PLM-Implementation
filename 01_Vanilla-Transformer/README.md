# PLM-Implementation: Vanilia-Transformer
## PyTorch Implementation of the Transformer model

<p align="center">
<img src="http://imgur.com/1krF2R6.png" width="250">
</p>

## References:
1. [Attention Is All You Need - Paper (arxiv)](https://arxiv.org/abs/1706.03762)
2. [**harvardnlp** - The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
